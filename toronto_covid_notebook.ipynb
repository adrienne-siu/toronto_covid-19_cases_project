{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto COVID-19 Cases Project\n",
    "\n",
    "Author: Adrienne Siu\n",
    "Date: August 2020\n",
    "\n",
    "# 1. Problem Definition\n",
    "\n",
    "In this project, I will use machine learning to:\n",
    "(1) Predict the outcomes of cases of COVID-19 in Toronto\n",
    "(2) Find the variables that correlate most with the outcome\n",
    "\n",
    "The dataset is available on Toronto Open Data and the version from July 29, 2020 was used: https://open.toronto.ca/dataset/covid-19-cases-in-toronto/\n",
    "\n",
    "This dataset has been saved as 'COVID19 cases.csv'.\n",
    "\n",
    "The three possible outcomes are: fatal, resolved (not fatal), and active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "%qtconsole\n",
    "import pdb\n",
    "#import contextlib\n",
    "#with contextlib.redirect_stdout(None):\n",
    "#import pixiedust\n",
    "\n",
    "#%%pixie_debugger\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, I will explore the data, including:\n",
    "(1) An overall view of the data and types of variables\n",
    "(2) splitting the data into the training and validation sets\n",
    "(3) finding correlations between variables and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall view of the data and types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import math as mt\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of dataset file (.csv)\n",
    "covid_file_path = 'COVID19 cases.csv'\n",
    "\n",
    "# Read the file\n",
    "covid_data = pd.read_csv(covid_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>Assigned_ID</th>\n",
       "      <th>Outbreak Associated</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Neighbourhood Name</th>\n",
       "      <th>FSA</th>\n",
       "      <th>Source of Infection</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Episode Date</th>\n",
       "      <th>Reported Date</th>\n",
       "      <th>Client Gender</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Currently Hospitalized</th>\n",
       "      <th>Currently in ICU</th>\n",
       "      <th>Currently Intubated</th>\n",
       "      <th>Ever Hospitalized</th>\n",
       "      <th>Ever in ICU</th>\n",
       "      <th>Ever Intubated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sporadic</td>\n",
       "      <td>50 to 59 Years</td>\n",
       "      <td>Willowdale East</td>\n",
       "      <td>M2N</td>\n",
       "      <td>Travel</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sporadic</td>\n",
       "      <td>50 to 59 Years</td>\n",
       "      <td>Willowdale East</td>\n",
       "      <td>M2N</td>\n",
       "      <td>Travel</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>MALE</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Sporadic</td>\n",
       "      <td>20 to 29 Years</td>\n",
       "      <td>Parkwoods-Donalda</td>\n",
       "      <td>M3A</td>\n",
       "      <td>Travel</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>2020-02-21</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Sporadic</td>\n",
       "      <td>60 to 69 Years</td>\n",
       "      <td>Church-Yonge Corridor</td>\n",
       "      <td>M4W</td>\n",
       "      <td>Travel</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Sporadic</td>\n",
       "      <td>60 to 69 Years</td>\n",
       "      <td>Church-Yonge Corridor</td>\n",
       "      <td>M4W</td>\n",
       "      <td>Travel</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>2020-02-20</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>MALE</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id  Assigned_ID Outbreak Associated       Age Group  \\\n",
       "0    1            1            Sporadic  50 to 59 Years   \n",
       "1    2            2            Sporadic  50 to 59 Years   \n",
       "2    3            3            Sporadic  20 to 29 Years   \n",
       "3    4            4            Sporadic  60 to 69 Years   \n",
       "4    5            5            Sporadic  60 to 69 Years   \n",
       "\n",
       "      Neighbourhood Name  FSA Source of Infection Classification Episode Date  \\\n",
       "0        Willowdale East  M2N              Travel      CONFIRMED   2020-01-22   \n",
       "1        Willowdale East  M2N              Travel      CONFIRMED   2020-01-21   \n",
       "2      Parkwoods-Donalda  M3A              Travel      CONFIRMED   2020-02-05   \n",
       "3  Church-Yonge Corridor  M4W              Travel      CONFIRMED   2020-02-16   \n",
       "4  Church-Yonge Corridor  M4W              Travel      CONFIRMED   2020-02-20   \n",
       "\n",
       "  Reported Date Client Gender   Outcome Currently Hospitalized  \\\n",
       "0    2020-01-23        FEMALE  RESOLVED                     No   \n",
       "1    2020-01-23          MALE  RESOLVED                     No   \n",
       "2    2020-02-21        FEMALE  RESOLVED                     No   \n",
       "3    2020-02-25        FEMALE  RESOLVED                     No   \n",
       "4    2020-02-26          MALE  RESOLVED                     No   \n",
       "\n",
       "  Currently in ICU Currently Intubated Ever Hospitalized Ever in ICU  \\\n",
       "0               No                  No                No          No   \n",
       "1               No                  No               Yes          No   \n",
       "2               No                  No                No          No   \n",
       "3               No                  No                No          No   \n",
       "4               No                  No                No          No   \n",
       "\n",
       "  Ever Intubated  \n",
       "0             No  \n",
       "1             No  \n",
       "2             No  \n",
       "3             No  \n",
       "4             No  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15338 entries, 0 to 15337\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   _id                     15338 non-null  int64 \n",
      " 1   Assigned_ID             15338 non-null  int64 \n",
      " 2   Outbreak Associated     15338 non-null  object\n",
      " 3   Age Group               15311 non-null  object\n",
      " 4   Neighbourhood Name      14729 non-null  object\n",
      " 5   FSA                     14775 non-null  object\n",
      " 6   Source of Infection     15338 non-null  object\n",
      " 7   Classification          15338 non-null  object\n",
      " 8   Episode Date            15338 non-null  object\n",
      " 9   Reported Date           15338 non-null  object\n",
      " 10  Client Gender           15338 non-null  object\n",
      " 11  Outcome                 15338 non-null  object\n",
      " 12  Currently Hospitalized  15338 non-null  object\n",
      " 13  Currently in ICU        15338 non-null  object\n",
      " 14  Currently Intubated     15338 non-null  object\n",
      " 15  Ever Hospitalized       15338 non-null  object\n",
      " 16  Ever in ICU             15338 non-null  object\n",
      " 17  Ever Intubated          15338 non-null  object\n",
      "dtypes: int64(2), object(16)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Look for categorical variables\n",
    "covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the patient ID, the columns are all categorical variables. I will deal with encoding for categorical variables in the modelling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Episode Date and Reported Date columns to datetimes\n",
    "# Input: data: dataframe\n",
    "# Output: dataframe with date columns as datetime objects\n",
    "def change_to_datetime(data):\n",
    "    data['Episode Date'] = pd.to_datetime(data['Episode Date'], format='%Y-%m-%d')\n",
    "    data['Reported Date'] = pd.to_datetime(data['Reported Date'], format='%Y-%m-%d')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Episode Date and Reported Date columns to datetimes\n",
    "covid_data = change_to_datetime(covid_data)\n",
    "\n",
    "#covid_data['Episode Date'].dt.strftime('%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicated entries\n",
    "covid_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target: outcome\n",
    "y = covid_data.Outcome\n",
    "\n",
    "# Predictor: drop _id, Assigned_ID, and Outcome columns\n",
    "X = covid_data.drop(['_id', 'Assigned_ID', 'Outcome'], axis=1)\n",
    "\n",
    "# Split into training and validation sets (80/20 split)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_excel(\"X_train.xlsx\")\n",
    "# X_valid.to_excel(\"X_valid.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe variables in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outbreak Associated</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Neighbourhood Name</th>\n",
       "      <th>FSA</th>\n",
       "      <th>Source of Infection</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Episode Date</th>\n",
       "      <th>Reported Date</th>\n",
       "      <th>Client Gender</th>\n",
       "      <th>Currently Hospitalized</th>\n",
       "      <th>Currently in ICU</th>\n",
       "      <th>Currently Intubated</th>\n",
       "      <th>Ever Hospitalized</th>\n",
       "      <th>Ever in ICU</th>\n",
       "      <th>Ever Intubated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12270</td>\n",
       "      <td>12247</td>\n",
       "      <td>11787</td>\n",
       "      <td>11824</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "      <td>12270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>154</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sporadic</td>\n",
       "      <td>50 to 59 Years</td>\n",
       "      <td>Glenfield-Jane Heights</td>\n",
       "      <td>M9V</td>\n",
       "      <td>N/A - Outbreak associated</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>2020-04-15 00:00:00</td>\n",
       "      <td>2020-05-29 00:00:00</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7681</td>\n",
       "      <td>1958</td>\n",
       "      <td>400</td>\n",
       "      <td>705</td>\n",
       "      <td>4589</td>\n",
       "      <td>11297</td>\n",
       "      <td>246</td>\n",
       "      <td>357</td>\n",
       "      <td>6490</td>\n",
       "      <td>12200</td>\n",
       "      <td>12255</td>\n",
       "      <td>12262</td>\n",
       "      <td>10757</td>\n",
       "      <td>11939</td>\n",
       "      <td>12036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-21 00:00:00</td>\n",
       "      <td>2020-01-23 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-07-27 00:00:00</td>\n",
       "      <td>2020-07-27 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Outbreak Associated       Age Group      Neighbourhood Name    FSA  \\\n",
       "count                12270           12247                   11787  11824   \n",
       "unique                   2               9                     140     96   \n",
       "top               Sporadic  50 to 59 Years  Glenfield-Jane Heights    M9V   \n",
       "freq                  7681            1958                     400    705   \n",
       "first                  NaN             NaN                     NaN    NaN   \n",
       "last                   NaN             NaN                     NaN    NaN   \n",
       "\n",
       "              Source of Infection Classification         Episode Date  \\\n",
       "count                       12270          12270                12270   \n",
       "unique                          8              2                  161   \n",
       "top     N/A - Outbreak associated      CONFIRMED  2020-04-15 00:00:00   \n",
       "freq                         4589          11297                  246   \n",
       "first                         NaN            NaN  2020-01-21 00:00:00   \n",
       "last                          NaN            NaN  2020-07-27 00:00:00   \n",
       "\n",
       "              Reported Date Client Gender Currently Hospitalized  \\\n",
       "count                 12270         12270                  12270   \n",
       "unique                  154             5                      2   \n",
       "top     2020-05-29 00:00:00        FEMALE                     No   \n",
       "freq                    357          6490                  12200   \n",
       "first   2020-01-23 00:00:00           NaN                    NaN   \n",
       "last    2020-07-27 00:00:00           NaN                    NaN   \n",
       "\n",
       "       Currently in ICU Currently Intubated Ever Hospitalized Ever in ICU  \\\n",
       "count             12270               12270             12270       12270   \n",
       "unique                2                   2                 2           2   \n",
       "top                  No                  No                No          No   \n",
       "freq              12255               12262             10757       11939   \n",
       "first               NaN                 NaN               NaN         NaN   \n",
       "last                NaN                 NaN               NaN         NaN   \n",
       "\n",
       "       Ever Intubated  \n",
       "count           12270  \n",
       "unique              2  \n",
       "top                No  \n",
       "freq            12036  \n",
       "first             NaN  \n",
       "last              NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        12270\n",
       "unique           3\n",
       "top       RESOLVED\n",
       "freq         11018\n",
       "Name: Outcome, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain filtered entries in dataset\n",
    "# Input: column_name is a string for the name of the column, e.g. 'Outbreak', 'Age Group'\n",
    "#        filter_column is a string for the column to be filtered\n",
    "#        filter_entry is a string for the entry to be filtered\n",
    "# Ex. To filter only the fatal outcomes, filter_column = 'Outcome' and filter_entry = 'FATAL'\n",
    "# Returns entries and value counts for the specific column after filtering\n",
    "def get_filtered_entries_and_value_counts_from_column(column_name, filter_column, filter_entry):\n",
    "    # Get the count of each unique entry (ordered by name)\n",
    "    entry_vc = X_train[X_train[filter_column]==filter_entry][column_name].value_counts() #.sort_index()\n",
    "    # Get the unique entries and put them in a list\n",
    "    entry = entry_vc.index.tolist()    \n",
    "    return entry, entry_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a bar chart of a column variable with filters applied\n",
    "# Stack by outcome: active, fatal, resolved\n",
    "# Input: column_name is a string for the name of the column, e.g. 'Outbreak', 'Age Group'\n",
    "#        filter_column is a string for the column to be filtered\n",
    "#        filter_entry_list is a list of strings for entries to be filtered\n",
    "def create_filtered_bar_chart(column_name, filter_column, filter_entry_list, fig_size):\n",
    "    # Initialize lists of entries and value counts after filtering\n",
    "    entry_list = []\n",
    "    entry_vc_list = []\n",
    "    \n",
    "    # Figure size\n",
    "    if fig_size == 'large':\n",
    "        fig = plt.figure(figsize=(20,30))\n",
    "        #plt.xticks(rotation=90)\n",
    "    elif fig_size == 'medium':\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "\n",
    "    # Loop through entries to be filtered\n",
    "    for i in range(len(filter_entry_list)):\n",
    "        # Entries and value counts for each filter\n",
    "        entry, entry_vc = get_filtered_entries_and_value_counts_from_column(column_name, filter_column, filter_entry_list[i])\n",
    "        # Append to list\n",
    "        entry_list.append(entry)\n",
    "        entry_vc_list.append(entry_vc)\n",
    "        # Create a bar stacked for each filtered entry, e.g. 'Fatal', 'Active', 'Resolved'\n",
    "        # Make vertical bar charts for Episode Date and Reported Date columns\n",
    "        if column_name == 'Episode Date' or column_name == 'Reported Date':\n",
    "            plt.bar(entry_list[i], entry_vc_list[i])\n",
    "            plt.xticks(rotation=45)\n",
    "        # Otherwise make horizontal bar charts\n",
    "        else:\n",
    "            plt.barh(entry_list[i], entry_vc_list[i])\n",
    "            # Invert the y-axis so that the order of the entries is from top to bottom\n",
    "            plt.gca().invert_yaxis()\n",
    "        \n",
    "    plt.title('Number of COVID-19 Cases By ' + column_name)\n",
    "    plt.legend(filter_entry_list)\n",
    "    plt.savefig('Plots/' + column_name + ' ' + str(i) + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Outcome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Outcome'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-26066360cd00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mfig_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'normal'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcreate_filtered_bar_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_entry_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-70ffdc7452ad>\u001b[0m in \u001b[0;36mcreate_filtered_bar_chart\u001b[0;34m(column_name, filter_column, filter_entry_list, fig_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_entry_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Entries and value counts for each filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_vc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filtered_entries_and_value_counts_from_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_entry_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Append to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mentry_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b45eaa808465>\u001b[0m in \u001b[0;36mget_filtered_entries_and_value_counts_from_column\u001b[0;34m(column_name, filter_column, filter_entry)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filtered_entries_and_value_counts_from_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Get the count of each unique entry (ordered by name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mentry_vc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfilter_entry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.sort_index()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get the unique entries and put them in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_vc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Outcome'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put the column names of the dataset into a list\n",
    "columns = X_train.columns\n",
    "\n",
    "# Remove the first two column names (the IDs) because I don't need to graph them\n",
    "columns_no_IDs = columns[2:]\n",
    "\n",
    "# Column to be filtered\n",
    "filter_column = 'Outcome'\n",
    "# Entries to be filtered:\n",
    "# First make stacked bar charts of all three outcomes, then bar charts of only fatal outcomes\n",
    "filter_entry_list = [['FATAL', 'RESOLVED', 'ACTIVE'], ['FATAL']]\n",
    "\n",
    "for j in range(len(filter_entry_list)):\n",
    "    # Create bar charts for relevant columns\n",
    "    for column in columns_no_IDs:\n",
    "        #create_bar_chart(column)\n",
    "        # Figure size\n",
    "        if column == 'Neighbourhood Name' or column == 'FSA': # or column == 'Episode Date' or column == 'Reported Date':\n",
    "            fig_size = 'large'\n",
    "        elif column == 'Episode Date' or column == 'Reported Date':\n",
    "            fig_size = 'medium'\n",
    "        else:\n",
    "            fig_size = 'normal'\n",
    "\n",
    "        create_filtered_bar_chart(column, filter_column, filter_entry_list[j], fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fatal cases make up 7.5% of total cases. As the number of resolved cases is an order of a magnitude higher, it is difficult to see the fatal cases in the bar charts. Hence, the fatal cases are graphed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "In this section, I will clean the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a table of missing values in columns\n",
    "# Function written by Nikos Tavoularis on Stack Overflow: https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe/39734251#39734251\n",
    "# Function comments by AS\n",
    "def missing_values_table(df):\n",
    "        # Count the number of missing values in each column\n",
    "        mis_val = df.isnull().sum()\n",
    "        # Calculate the percentage of missing values in each column\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        # Put the number of missing values and % missing values in a table\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        # Rename the columns of the table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        # Take out columns with no missing values\n",
    "        # Reorder columns with missing values in descending order\n",
    "        # Round % missing values to 1 decimal place\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        # Print summary\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        # Return table of missing values\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# Create table of missing values\n",
    "missing_values_table(covid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, there are only 3/18 columns with missing values. For each column, the % missing values is 4% or less. Hence, the dataset is fairly complete.\n",
    "\n",
    "I will fill in the missing values as follows:\n",
    "\n",
    "-- If both the neighbourhood name and FSA are blank, find the modal FSA and neighbourhood name (as a pair).\n",
    "Note I am not using SimpleImputer because I would like to consider the relationship between the neighbourhood name and FSA. If the modal neighbourhood name and modal FSA are treated separately as in SimpleImputer, the resulting pair\n",
    "of neighbourhood name and FSA could be impossible in real life.\n",
    "\n",
    "-- If the neighbourhood name is blank but the FSA is known, find the modal neighbourhood name for the given FSA.\n",
    "\n",
    "-- Note that there are no cases of the FSA being blank but the neighbourhood name is known.\n",
    "\n",
    "-- The age group will be filled with modal imputation, i.e. the modal age group. Note that the median and the modal age group in the training set are the same, so median imputation would also yield the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for neighbourhood name and FSA imputation\n",
    "# If both the neighbourhood and FSA are blank, find the modal FSA and neighbourhood name (as a pair)\n",
    "# Input: dataset for imputation\n",
    "# Output: modal FSA and neighbourhood name, sorted dataset\n",
    "def find_mode_fsa_nn(data):\n",
    "    # Group data by FSA and neighbourhood and count number of occurrences\n",
    "    fsa_nn_group_data = data.groupby(['FSA', 'Neighbourhood Name'])['Outbreak Associated'].agg([len]).reset_index()\n",
    "    # Sort from highest to lowest count (i.e. the mode is the first value)\n",
    "    fsa_nn_group_data_sorted = fsa_nn_group_data.sort_values(by='len', ascending=False)\n",
    "    # Find modal neighbourhood name and FSA\n",
    "    mode_fsa = fsa_nn_group_data_sorted.iloc[0]['FSA']\n",
    "    mode_nn = fsa_nn_group_data_sorted.iloc[0]['Neighbourhood Name']\n",
    "    return mode_fsa, mode_nn, fsa_nn_group_data_sorted\n",
    "\n",
    "# Impute missing values for the neighbourhood name and FSA\n",
    "# Note there are no cases when the neighbourhood name is known but the FSA is blank\n",
    "# Input: dataset, sorted dataset\n",
    "def impute_missing_fsa_nn(data, mode_fsa, mode_nn, fsa_nn_group_data_sorted):\n",
    "    # If an entry is blank, it is a 'NaN' and hence a float\n",
    "    # If an entry is not blank, it is a string\n",
    "    for i in data.index:\n",
    "         # If both the neighbourhood name and FSA are blank\n",
    "        if type(data['Neighbourhood Name'][i]) == float and type(data['FSA'][i]) == float:\n",
    "            # Use the modal FSA and neighbourhood name\n",
    "            data['FSA'][i] = mode_fsa\n",
    "            data['Neighbourhood Name'][i] = mode_nn\n",
    "            #print(\"Instance 1\", i, data['FSA'][i], data['Neighbourhood Name'][i])\n",
    "\n",
    "         # If the neighbourhood name is blank but the FSA is known\n",
    "        if type(data['Neighbourhood Name'][i]) == float and type(data['FSA'][i]) != float:\n",
    "            # Find the modal neighbourhood name for the given FSA\n",
    "            # i.e. find the first instance of FSA in fsa_nn_group_data_sorted and its associated neighbourhood name\n",
    "            mode_nn_for_fsa = fsa_nn_group_data_sorted.loc[fsa_nn_group_data_sorted['FSA'] == data['FSA'][i]].iloc[0]\n",
    "            data['Neighbourhood Name'][i] = mode_nn_for_fsa['Neighbourhood Name']\n",
    "            #print(\"Instance 2\", i, data['Neighbourhood Name'][i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new copies of datasets\n",
    "# y doesn't have any missing values, so imputation is not needed\n",
    "X_train_imp_area = X_train.copy()\n",
    "X_valid_imp_area = X_valid.copy()\n",
    "\n",
    "# On the training set, return the mode FSA, mode neighbourhood name, and sorted training set\n",
    "mode_fsa, mode_nn, fsa_nn_group_data_sorted = find_mode_fsa_nn(X_train_imp_area)\n",
    "\n",
    "# Impute missing FSA and neighbourhood name values in the training set\n",
    "X_train_imp_area = impute_missing_fsa_nn(X_train_imp_area, mode_fsa, mode_nn, fsa_nn_group_data_sorted)\n",
    "\n",
    "# Impute missing FSA and neighbourhood name values in the validation set based on the modes\n",
    "# in the training set\n",
    "X_valid_imp_area = impute_missing_fsa_nn(X_valid_imp_area, mode_fsa, mode_nn, fsa_nn_group_data_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age group: modal imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert all columns to strings to avoid error with data type\n",
    "X_train_new = X_train_imp_area.applymap(str) # Convert blanks to 'nan'\n",
    "X_valid_new = X_valid_imp_area.applymap(str) \n",
    "\n",
    "# Imputer with most_frequent (modal) strategy (note this removes the original indices of the entries)\n",
    "imputer = SimpleImputer(missing_values='nan', strategy='most_frequent', verbose=1)\n",
    "X_train_imp = pd.DataFrame(imputer.fit_transform(X_train_new)) \n",
    "X_valid_imp = pd.DataFrame(imputer.transform(X_valid_new))\n",
    "\n",
    "# Rename columns\n",
    "X_train_imp.columns = X_train_imp_area.columns\n",
    "X_valid_imp.columns = X_valid_imp_area.columns\n",
    "\n",
    "# Put back the original indices\n",
    "X_train_imp.index = X_train_imp_area.index\n",
    "X_valid_imp.index = X_valid_imp_area.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsa_nn_group_data_sorted.to_excel(\"fsa_nn_group_data_sorted.xlsx\")\n",
    "# X_train_imp_area.to_excel(\"X_train_imp_area.xlsx\")\n",
    "# X_valid_imp_area.to_excel(\"X_valid_imp_area.xlsx\")\n",
    "# X_train_imp.to_excel(\"X_train_imp.xlsx\")\n",
    "# X_valid_imp.to_excel(\"X_valid_imp.xlsx\")\n",
    "# X_train_new.to_excel(\"X_train_new.xlsx\")\n",
    "# X_valid_new.to_excel(\"X_valid_new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values_table(X_train_imp)\n",
    "missing_values_table(X_valid_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection and Engineering\n",
    "\n",
    "Feature engineering will involve encoding the features:\n",
    "\n",
    "-- Outbreak Associated (2 values): one-hot encoding with binary values, equivalent to label encoding (0/1) to avoid the dummy variable trap\n",
    "\n",
    "-- Age Group (9 values): label encoding\n",
    "\n",
    "-- Neighbourhood Name and FSA: one-hot encoding with 'Other' column for rare cases\n",
    "\n",
    "-- Source of Infection (8 values): one-hot encoding\n",
    "\n",
    "-- Classification (2 values): one-hot encoding with binary values, equivalent to label encoding (0/1)\n",
    "\n",
    "-- Episode Date and Reported Date: encode month, date, day of the week as cyclical features (other possibilities: time since interesting event? holiday? season?)\n",
    "\n",
    "-- Client Gender (5 values): one-hot encoding\n",
    "\n",
    "-- Currently Hospitalized, Currently in ICU, Currently Intubated, Ever Hospitalized, Ever in ICU, Ever Intubated (2 values): one-hot encoding with binary values, equivalent to label encoding (0/1)\n",
    "\n",
    "CONSIDER SCALING AT THE END (FOR THE AGE GROUP)\n",
    "\n",
    "Feature selection will involve removing unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outbreak Associated, Age Group, Currently Hospitalized, Currently in ICU, Currently Intubated, \n",
    "# Ever Hospitalized, Ever in ICU, Ever Intubated: label encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Specify columns for label encoding\n",
    "lbl_enc_col = ['Outbreak Associated', 'Age Group', 'Classification', 'Currently Hospitalized', 'Currently in ICU', \\\n",
    "               'Currently Intubated', 'Ever Hospitalized', 'Ever in ICU', 'Ever Intubated']\n",
    "\n",
    "# Define the label encoder\n",
    "lbl_enc = LabelEncoder()\n",
    "\n",
    "# Make a copy\n",
    "X_train_lbl = X_train_imp.copy()\n",
    "X_valid_lbl = X_valid_imp.copy()\n",
    "\n",
    "# Loop through all of the specified columns\n",
    "for col in lbl_enc_col:\n",
    "    X_train_lbl[col] = lbl_enc.fit_transform(X_train_lbl[col])\n",
    "    X_valid_lbl[col] = lbl_enc.transform(X_valid_lbl[col])\n",
    "    \n",
    "# Put back indices\n",
    "X_train_lbl.index = X_train_imp.index\n",
    "X_valid_lbl.index = X_valid_imp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neighbourhood Name and FSA: one-hot encoding with 'Other' column for rare cases\n",
    "# # Based on Maya Gilad's tutorial here: https://medium.com/gett-engineering/handling-rare-categorical-values-in-pandas-d1e3f17475f0\n",
    "\n",
    "# # Value counts (number of cases associated with each FSA)\n",
    "# X_train_oh['FSA'].value_counts()\n",
    "# # Statistics\n",
    "# X_train_oh['FSA'].value_counts().describe()\n",
    "# # Normalized value counts\n",
    "# X_train_oh['FSA'].value_counts(normalize=True)\n",
    "# # Normalized value counts: statistics\n",
    "# X_train_oh['FSA'].value_counts(normalize=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Value counts (number of cases associated with each Neighbourhood Name)\n",
    "# X_train_oh['Neighbourhood Name'].value_counts()\n",
    "# # Statistics\n",
    "# X_train_oh['Neighbourhood Name'].value_counts().describe()\n",
    "# # Normalized value counts\n",
    "# X_train_oh['Neighbourhood Name'].value_counts(normalize=True)\n",
    "# # Normalized value counts: statistics\n",
    "# X_train_oh['Neighbourhood Name'].value_counts(normalize=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the threshold for specifying rare cases and the normalized value counts (in descending order)\n",
    "# Input: col_name: column name\n",
    "# Note that this is done on the training set only (X_train_oh)\n",
    "def get_threshold_and_norm_vc_desc(col_name):    \n",
    "    # Normalized value counts\n",
    "    norm_vc = X_train_oh[col_name].value_counts(normalize=True, ascending=True)\n",
    "\n",
    "    # Inferred threshold\n",
    "    # Having 10% of outcomes in the 'Other' category seems to be a good starting point, so I will use this\n",
    "    # to set the threshold\n",
    "    threshold = norm_vc[(norm_vc.cumsum() > 0.1).idxmax()]\n",
    "\n",
    "    # Normalized value counts (in descending order)\n",
    "    norm_vc_desc = X_train_oh[col_name].value_counts(normalize=True)\n",
    "    \n",
    "    return threshold, norm_vc_desc\n",
    "\n",
    "# Function to replace the column with rare cases marked as 'Other'\n",
    "# Input: data: either the training set (X_train_oh) or the validation set (X_valid_oh)\n",
    "#        thresh_fsa, thresh_nn: threshold for marking rare case (from the training set)\n",
    "#        norm_vc_desc_fsa, norm_vc_desc_nn: normalized value counts in descending order (from the training set)\n",
    "# Output: data with modified column\n",
    "def replace_rare_cases_in_col(data, thresh_fsa, thresh_nn, norm_vc_desc_fsa, norm_vc_desc_nn):\n",
    "    # FSA\n",
    "    # Map the column to its normalized value count\n",
    "    map_to_norm_vc_desc_fsa = data['FSA'].map(norm_vc_desc_fsa)\n",
    "    # Replace the column entry with 'Other' when the normalized value count is less than the threshold\n",
    "    data['FSA'] = data['FSA'].mask(map_to_norm_vc_desc_fsa < thresh_fsa, 'Other')\n",
    "\n",
    "    # Neighbourhood Name\n",
    "    # Map the column to its normalized value count\n",
    "    map_to_norm_vc_desc_nn = data['Neighbourhood Name'].map(norm_vc_desc_nn)\n",
    "    # Replace the column entry with 'Other' when the normalized value count is less than the threshold\n",
    "    data['Neighbourhood Name'] = data['Neighbourhood Name'].mask(map_to_norm_vc_desc_nn < thresh_nn, 'Other')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neighbourhood Name and FSA: one-hot encoding with 'Other' column for rare cases\n",
    "# # Based on Maya Gilad's tutorial here: https://medium.com/gett-engineering/handling-rare-categorical-values-in-pandas-d1e3f17475f0\n",
    "\n",
    "# FSA and neighbourhood name are treated separately\n",
    "\n",
    "# Make copies of the dataframes\n",
    "X_train_oh = X_train_lbl.copy()\n",
    "X_valid_oh = X_valid_lbl.copy()\n",
    "\n",
    "# Get thresholds and normalized value counts from the training set\n",
    "thresh_fsa, norm_vc_desc_fsa = get_threshold_and_norm_vc_desc('FSA')\n",
    "thresh_nn, norm_vc_desc_nn = get_threshold_and_norm_vc_desc('Neighbourhood Name')\n",
    "\n",
    "# Get training set and validation set with rare column entries listed as 'Other'\n",
    "X_train_oh = replace_rare_cases_in_col(X_train_oh, thresh_fsa, thresh_nn, norm_vc_desc_fsa, norm_vc_desc_nn)\n",
    "X_valid_oh = replace_rare_cases_in_col(X_valid_oh, thresh_fsa, thresh_nn, norm_vc_desc_fsa, norm_vc_desc_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine rare cases of the FSA and neighbourhood name\n",
    "# # A patient's location is defined by a combination of FSA and neighbourhood name.\n",
    "# # FSA and neighbourhood name are treated as a set, i.e. the number of occurrences of a particular combination of\n",
    "# # FSA and neighbourhood name are found. \n",
    "# # Otherwise, if the FSA and NN are treated independently, you may have cases where one of the FSA and NN are \n",
    "# # labelled 'Other', but not both.\n",
    "# # The threshold for determining 'rare' cases is such that at least 10% of cases are rare. \n",
    "\n",
    "# # Make a copy of the dataset\n",
    "# X_train_other = X_train_oh.copy()\n",
    "# X_valid_other = X_valid_oh.copy()\n",
    "\n",
    "# # For the training set: group data by FSA and neighbourhood and count number of occurrences\n",
    "# fsa_nn = X_train_other.groupby(['Neighbourhood Name', 'FSA'])['Outbreak Associated'].agg([len]).reset_index()\n",
    "# # Sort from lowest to highest count\n",
    "# fsa_nn_sorted = fsa_nn.sort_values(by='len', ascending=True)\n",
    "# # Normalize the value counts (or length)\n",
    "# norm_vc = fsa_nn_sorted['len']/(np.sum(fsa_nn_sorted['len']))\n",
    "# ## Map the normalized value counts to the sorted dataframe\n",
    "# #fsa_nn_map = fsa_nn_sorted['FSA'].map(norm_vc)\n",
    "# # Add a new column with normalized value counts\n",
    "# fsa_nn_sorted['norm_vc'] = norm_vc\n",
    "\n",
    "# # Inferred threshold\n",
    "# # Having 10% of outcomes in the 'Other' category seems to be a good starting point, so I will use this\n",
    "# # to set the threshold\n",
    "# thresh_fsa_nn = norm_vc.loc[(norm_vc.cumsum() > 0.1).idxmax()]\n",
    "\n",
    "# # Normalized value counts (in descending order)\n",
    "# fsa_nn_sorted_desc = fsa_nn_sorted.sort_values(by='norm_vc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to return dataframe with rare FSA and Neighbourhood Name cases as 'Other'\n",
    "# # Output: modified dataframe (either training set or validation set)\n",
    "# # Input: data: original dataframe (either training set or validation set)\n",
    "# #        fsa_nn_sorted_desc: dataframe with sorted FSA and NN value counts\n",
    "# #        thresh_fsa_nn: normalized-value-count threshold for 'Other' cases\n",
    "# def data_with_fsa_nn_other(data, fsa_nn_sorted_desc, thresh_fsa_nn):\n",
    "#     # Merge the training data with the normalized value counts\n",
    "#     norm_vc_merge = data[['Neighbourhood Name', 'FSA']].merge(fsa_nn_sorted_desc, how='left', indicator=True)['norm_vc']\n",
    "#     # Make the indices the same (norm_vc is ordered by index)\n",
    "#     norm_vc_merge.index = data.index\n",
    "#     # Add the norm_vc column to the training data\n",
    "#     data['norm_vc'] = norm_vc_merge\n",
    "#     # Replace the Neighbourhood Name and FSA with 'Other' when the normalized value count is less than the threshold\n",
    "#     data[['Neighbourhood Name', 'FSA']] = data[['Neighbourhood Name', 'FSA']].mask(data['norm_vc'] < thresh_fsa_nn, 'Other')\n",
    "#     # Remove norm_vc column\n",
    "#     data = data.drop(['norm_vc'], axis=1)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataframes with rare FSA and Neighbourhood Name cases as 'Other'\n",
    "# # Use normalized value counts and threshold for the FSA and NN from the training set and apply to the validation set\n",
    "# X_train_other = data_with_fsa_nn_other(X_train_other, fsa_nn_sorted_desc, thresh_fsa_nn)\n",
    "# X_valid_other = data_with_fsa_nn_other(X_valid_other, fsa_nn_sorted_desc, thresh_fsa_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbourhood Name, FSA, Source of Infection, Client Gender: one-hot encoding\n",
    "# FOR TREE-BASED METHODS, VARIABLES SHOULDN'T BE DROPPED...\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Specify columns for one-hot encoding\n",
    "oh_col = ['Neighbourhood Name', 'FSA', 'Source of Infection', 'Client Gender']\n",
    "\n",
    "# One-hot encoder:\n",
    "# Return an array instead of a sparse matrix\n",
    "# Drop last value to avoid the dummy variable trap and also because the unknown columns aren't very helpful\n",
    "oh_enc = OneHotEncoder(handle_unknown='error', sparse=False, \\\n",
    "                       drop=[['Other'], ['Other'], ['Unknown/Missing'], ['UNKNOWN']])\n",
    "X_train_oh_col = pd.DataFrame(oh_enc.fit_transform(X_train_oh[oh_col]))\n",
    "X_valid_oh_col = pd.DataFrame(oh_enc.transform(X_valid_oh[oh_col]))\n",
    "\n",
    "# Put back the indices\n",
    "X_train_oh_col.index = X_train_lbl.index\n",
    "X_valid_oh_col.index = X_valid_lbl.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename one-hot encoded columns, dropping last value\n",
    "# Output: return dataframe with renamed columns\n",
    "# Input: X_train_oh_col or X_valid_oh_col (one-hot encoded columns);\n",
    "#        oh_enc (one-hot encoder)\n",
    "def rename_col_oh(X_oh_col, oh_enc):\n",
    "    # Concatenate column names, omitting 'Other' or 'Unknown' column names\n",
    "    X_oh_col.columns = np.concatenate((oh_enc.categories_[0][:-30],  # Neighbourhood Name (before 'Other' at index -30)\n",
    "                                       oh_enc.categories_[0][-29:],  # Neighbourhood Name (after 'Other')\n",
    "                                       oh_enc.categories_[1][:-1],   # FSA\n",
    "                                       oh_enc.categories_[2][:-1],   # Source of Infection\n",
    "                                       oh_enc.categories_[3][:-1]))  # Client Gender\n",
    "    return X_oh_col\n",
    "\n",
    "# Function to put the one-hot encoded columns back into the dataframe\n",
    "# Keep the original order for now\n",
    "# Output: return ordered dataframe\n",
    "# Input: X_train_oh or X_valid_oh (dataframe to modify); X_train_oh_col or X_valid_oh_col (one-hot encoded columns);\n",
    "#        oh_enc (one-hot encoder)\n",
    "def order_col_oh(X_oh, X_oh_col, oh_enc):\n",
    "    X_oh = pd.concat([X_oh.loc[:,'Outbreak Associated':'Age Group'], \n",
    "    X_oh_col.loc[:,oh_enc.categories_[0][0]:oh_enc.categories_[0][-31]],   # Neighbourhood Name (before 'Other')\n",
    "    X_oh_col.loc[:,oh_enc.categories_[0][-29]:oh_enc.categories_[0][-1]],  # Neighbourhood Name (after 'Other')\n",
    "    X_oh_col.loc[:,oh_enc.categories_[1][0]:oh_enc.categories_[1][-2]],    # FSA\n",
    "    X_oh_col.loc[:,oh_enc.categories_[2][0]:oh_enc.categories_[2][-2]],    # Source of Infection\n",
    "    X_oh.loc[:,'Classification':'Reported Date'], \n",
    "    X_oh_col.loc[:,oh_enc.categories_[3][0]:oh_enc.categories_[3][-2]],    # Client Gender\n",
    "    X_oh.loc[:,'Currently Hospitalized':'Ever Intubated']], axis=1, sort=False)\n",
    "    return X_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename one-hot encoded columns, dropping last value\n",
    "X_train_oh_col = rename_col_oh(X_train_oh_col, oh_enc)\n",
    "X_valid_oh_col = rename_col_oh(X_valid_oh_col, oh_enc)\n",
    "\n",
    "# Put the one-hot encoded columns back into the dataframe\n",
    "X_train_oh = order_col_oh(X_train_oh, X_train_oh_col, oh_enc)\n",
    "X_valid_oh = order_col_oh(X_valid_oh, X_valid_oh_col, oh_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode Date and Reported Date: encoding for dates\n",
    "\n",
    "# Change Episode Date and Reported Date columns to datetimes\n",
    "X_train_tmp = change_to_datetime(X_train_oh)\n",
    "X_valid_tmp = change_to_datetime(X_valid_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to extract the month from a dataframe column\n",
    "# # Input: data: either training or validation set dataframe\n",
    "# # Output: returns the array with extracted months\n",
    "# def extract_month(data):\n",
    "#     # Extract in the form of an integer\n",
    "#     data['Episode Date'] = data['Episode Date'].dt.strftime('%m').astype(int)\n",
    "#     data['Reported Date'] = data['Reported Date'].dt.strftime('%m').astype(int)\n",
    "\n",
    "#     # Rename columns\n",
    "#     data = data.rename(columns = {'Episode Date':'Episode Month', 'Reported Date':'Reported Month'})\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract month\n",
    "# X_train_month = extract_month(X_train_date)\n",
    "# X_valid_month = extract_month(X_valid_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with just the date info\n",
    "X_train_ep = X_train_tmp.loc[:,'Episode Date':'Reported Date']\n",
    "\n",
    "# TO DO: \n",
    "# INVESTIGATE IF REPORTED DATE IS A USEFUL FEATURE. IF SO, DO THE SAME FOR THE REPORTED DATE\n",
    "# DO THE SAME FOR THE VALIDATION SET\n",
    "# DETERMINE IF OTHER DATE FEATURES ARE NEEDED, E.G. TIME SINCE LOCKDOWN/PHASE 1,2,3\n",
    "\n",
    "# Extract month\n",
    "X_train_ep['Episode Month'] = X_train_ep['Episode Date'].dt.strftime('%m').astype(int)\n",
    "\n",
    "# Make month cyclical\n",
    "X_train_ep['Episode Month Sin'] = np.sin((X_train_ep['Episode Month']-1)*(2*np.pi/12))\n",
    "X_train_ep['Episode Month Cos'] = np.cos((X_train_ep['Episode Month']-1)*(2*np.pi/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of the month\n",
    "X_train_ep['Episode Day'] = X_train_ep['Episode Date'].dt.strftime('%d').astype(int)\n",
    "\n",
    "# Make day of the month cyclical\n",
    "# 31 days is the max number of days in a month (don't account for differences in months)\n",
    "X_train_ep['Episode Day Sin'] = np.sin((X_train_ep['Episode Day']-1)*(2*np.pi/31))\n",
    "X_train_ep['Episode Day Cos'] = np.cos((X_train_ep['Episode Day']-1)*(2*np.pi/31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of the week\n",
    "# Sunday is Day 0, Saturday is Day 6\n",
    "X_train_ep['Episode Weekday'] = X_train_ep['Episode Date'].dt.strftime('%w').astype(int)\n",
    "\n",
    "# Make day of the month cyclical\n",
    "# Days numbered from 0, so don't need to subtract 1\n",
    "# Use 7 in the denominator because there are 7 days in a week\n",
    "X_train_ep['Episode Weekday Sin'] = np.sin((X_train_ep['Episode Weekday'])*(2*np.pi/7))\n",
    "X_train_ep['Episode Weekday Cos'] = np.cos((X_train_ep['Episode Weekday'])*(2*np.pi/7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
