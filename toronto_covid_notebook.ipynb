{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto COVID-19 Cases Project\n",
    "\n",
    "Author: Adrienne Siu\n",
    "Date: August 2020\n",
    "\n",
    "# 1. Problem Definition\n",
    "\n",
    "In this project, I will use machine learning to:\n",
    "(1) Predict the outcomes of cases of COVID-19 in Toronto\n",
    "(2) Find the variables that correlate most with the outcome\n",
    "\n",
    "The dataset is available on Toronto Open Data and the version from July 29, 2020 was used: https://open.toronto.ca/dataset/covid-19-cases-in-toronto/\n",
    "\n",
    "This dataset has been saved as 'COVID19 cases.csv'.\n",
    "\n",
    "The three possible outcomes are: fatal, resolved (not fatal), and active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "%qtconsole\n",
    "import pdb\n",
    "#import contextlib\n",
    "#with contextlib.redirect_stdout(None):\n",
    "#import pixiedust\n",
    "\n",
    "#%%pixie_debugger\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, I will explore the data, including correlations between variables and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of dataset file (.csv)\n",
    "covid_file_path = 'COVID19 cases.csv'\n",
    "\n",
    "# Read the file\n",
    "covid_data = pd.read_csv(covid_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain filtered entries in dataset\n",
    "# Input: column_name is a string for the name of the column, e.g. 'Outbreak', 'Age Group'\n",
    "#        filter_column is a string for the column to be filtered\n",
    "#        filter_entry is a string for the entry to be filtered\n",
    "# Ex. To filter only the fatal outcomes, filter_column = 'Outcome' and filter_entry = 'FATAL'\n",
    "# Returns entries and value counts for the specific column after filtering\n",
    "def get_filtered_entries_and_value_counts_from_column(column_name, filter_column, filter_entry):\n",
    "    # Get the count of each unique entry (ordered by name)\n",
    "    entry_vc = covid_data[covid_data[filter_column]==filter_entry][column_name].value_counts() #.sort_index()\n",
    "    # Get the unique entries and put them in a list\n",
    "    entry = entry_vc.index.tolist()    \n",
    "    return entry, entry_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a bar chart of a column variable with filters applied\n",
    "# Stack by outcome: active, fatal, resolved\n",
    "# Input: column_name is a string for the name of the column, e.g. 'Outbreak', 'Age Group'\n",
    "#        filter_column is a string for the column to be filtered\n",
    "#        filter_entry_list is a list of strings for entries to be filtered\n",
    "def create_filtered_bar_chart(column_name, filter_column, filter_entry_list, fig_size):\n",
    "    # Initialize lists of entries and value counts after filtering\n",
    "    entry_list = []\n",
    "    entry_vc_list = []\n",
    "    \n",
    "    # Figure size\n",
    "    if fig_size == 'large':\n",
    "        fig = plt.figure(figsize=(20,30))\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "\n",
    "    # Loop through entries to be filtered\n",
    "    for i in range(len(filter_entry_list)):\n",
    "        # Entries and value counts for each filter\n",
    "        entry, entry_vc = get_filtered_entries_and_value_counts_from_column(column_name, filter_column, filter_entry_list[i])\n",
    "        # Append to list\n",
    "        entry_list.append(entry)\n",
    "        entry_vc_list.append(entry_vc)\n",
    "        # Create a bar stacked for each filtered entry, e.g. 'Fatal', 'Active', 'Resolved'\n",
    "        plt.bar(entry_list[i], entry_vc_list[i])\n",
    "        #plt.barh(entry_list[i], entry_vc_list[i])\n",
    "\n",
    "    # Invert the y-axis so that the order of the entries is from top to bottom\n",
    "    #plt.gca().invert_yaxis()\n",
    "    plt.title('Number of COVID-19 Cases By ' + column_name)\n",
    "    plt.legend(filter_entry_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the column names of the dataset into a list\n",
    "columns = covid_data.columns\n",
    "\n",
    "# Remove the first two column names (the IDs) because I don't need to graph them\n",
    "columns_no_IDs = columns[2:]\n",
    "\n",
    "# Column to be filtered\n",
    "filter_column = 'Outcome'\n",
    "# Entries to be filtered:\n",
    "# First make stacked bar charts of all three outcomes, then bar charts of only fatal outcomes\n",
    "filter_entry_list = [['FATAL', 'RESOLVED', 'ACTIVE'], ['FATAL']]\n",
    "\n",
    "for j in range(len(filter_entry_list)):\n",
    "    # Create bar charts for relevant columns\n",
    "    for column in columns_no_IDs:\n",
    "        #create_bar_chart(column)\n",
    "        # Figure size\n",
    "        if column == 'Neighbourhood Name' or column == 'FSA' or column == 'Episode Date' or column == 'Reported Date':\n",
    "            fig_size = 'large'\n",
    "        else:\n",
    "            fig_size = 'normal'\n",
    "\n",
    "        create_filtered_bar_chart(column, filter_column, filter_entry_list[j], fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fatal cases make up 7.5% of total cases. As the number of resolved cases is an order of a magnitude higher, it is difficult to see the fatal cases in the bar charts. Hence, the fatal cases are graphed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "In this section, I will clean the data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target: outcome\n",
    "y = covid_data.Outcome\n",
    "\n",
    "# Predictor: drop _id, Assigned_ID, and Outcome columns\n",
    "X = covid_data.drop(['_id', 'Assigned_ID', 'Outcome'], axis=1)\n",
    "\n",
    "# Split into training and validation sets (80/20 split)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_excel(\"X_train.xlsx\")\n",
    "# X_valid.to_excel(\"X_valid.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a table of missing values in columns\n",
    "# Function written by Nikos Tavoularis on Stack Overflow: https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe/39734251#39734251\n",
    "# Function comments by AS\n",
    "def missing_values_table(df):\n",
    "        # Count the number of missing values in each column\n",
    "        mis_val = df.isnull().sum()\n",
    "        # Calculate the percentage of missing values in each column\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        # Put the number of missing values and % missing values in a table\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        # Rename the columns of the table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        # Take out columns with no missing values\n",
    "        # Reorder columns with missing values in descending order\n",
    "        # Round % missing values to 1 decimal place\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        # Print summary\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        # Return table of missing values\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# Create table of missing values\n",
    "missing_values_table(covid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, there are only 3/18 columns with missing values. For each column, the % missing values is 4% or less. Hence, the dataset is fairly complete.\n",
    "\n",
    "I will fill in the missing values as follows:\n",
    "\n",
    "-- If both the neighbourhood name and FSA are blank, find the modal FSA and neighbourhood name (as a pair).\n",
    "Note I am not using SimpleImputer because I would like to consider the relationship between the neighbourhood name and FSA. If the modal neighbourhood name and modal FSA are treated separately as in SimpleImputer, the resulting pair\n",
    "of neighbourhood name and FSA could be impossible in real life.\n",
    "\n",
    "-- If the neighbourhood name is blank but the FSA is known, find the modal neighbourhood name for the given FSA.\n",
    "\n",
    "-- Note that there are no cases of the FSA being blank but the neighbourhood name is known.\n",
    "\n",
    "-- The age group will be filled with modal imputation, i.e. the modal age group. Note that the median and the modal age group in the training set are the same, so median imputation would also yield the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for neighbourhood name and FSA imputation\n",
    "# If both the neighbourhood and FSA are blank, find the modal FSA and neighbourhood name (as a pair)\n",
    "# Input: dataset for imputation\n",
    "# Output: modal FSA and neighbourhood name, sorted dataset\n",
    "def find_mode_fsa_nn(data):\n",
    "    # Group data by FSA and neighbourhood and count number of occurrences\n",
    "    fsa_nn_group_data = data.groupby(['FSA', 'Neighbourhood Name'])['Outbreak Associated'].agg([len]).reset_index()\n",
    "    # Sort from highest to lowest count (i.e. the mode is the first value)\n",
    "    fsa_nn_group_data_sorted = fsa_nn_group_data.sort_values(by='len', ascending=False)\n",
    "    # Find modal neighbourhood name and FSA\n",
    "    mode_fsa = fsa_nn_group_data_sorted.iloc[0]['FSA']\n",
    "    mode_nn = fsa_nn_group_data_sorted.iloc[0]['Neighbourhood Name']\n",
    "    return mode_fsa, mode_nn, fsa_nn_group_data_sorted\n",
    "\n",
    "# Impute missing values for the neighbourhood name and FSA\n",
    "# Note there are no cases when the neighbourhood name is known but the FSA is blank\n",
    "# Input: dataset, sorted dataset\n",
    "def impute_missing_fsa_nn(data, mode_fsa, mode_nn, fsa_nn_group_data_sorted):\n",
    "    # If an entry is blank, it is a 'NaN' and hence a float\n",
    "    # If an entry is not blank, it is a string\n",
    "    for i in data.index:\n",
    "         # If both the neighbourhood name and FSA are blank\n",
    "        if type(data['Neighbourhood Name'][i]) == float and type(data['FSA'][i]) == float:\n",
    "            # Use the modal FSA and neighbourhood name\n",
    "            data['FSA'][i] = mode_fsa\n",
    "            data['Neighbourhood Name'][i] = mode_nn\n",
    "            #print(\"Instance 1\", i, data['FSA'][i], data['Neighbourhood Name'][i])\n",
    "\n",
    "         # If the neighbourhood name is blank but the FSA is known\n",
    "        if type(data['Neighbourhood Name'][i]) == float and type(data['FSA'][i]) != float:\n",
    "            # Find the modal neighbourhood name for the given FSA\n",
    "            # i.e. find the first instance of FSA in fsa_nn_group_data_sorted and its associated neighbourhood name\n",
    "            mode_nn_for_fsa = fsa_nn_group_data_sorted.loc[fsa_nn_group_data_sorted['FSA'] == data['FSA'][i]].iloc[0]\n",
    "            data['Neighbourhood Name'][i] = mode_nn_for_fsa['Neighbourhood Name']\n",
    "            #print(\"Instance 2\", i, data['Neighbourhood Name'][i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new copies of datasets\n",
    "# y doesn't have any missing values, so imputation is not needed\n",
    "X_train_imp_area = X_train.copy()\n",
    "X_valid_imp_area = X_valid.copy()\n",
    "\n",
    "# On the training set, return the mode FSA, mode neighbourhood name, and sorted training set\n",
    "mode_fsa, mode_nn, fsa_nn_group_data_sorted = find_mode_fsa_nn(X_train_imp_area)\n",
    "\n",
    "# Impute missing FSA and neighbourhood name values in the training set\n",
    "X_train_imp_area = impute_missing_fsa_nn(X_train_imp_area, mode_fsa, mode_nn, fsa_nn_group_data_sorted)\n",
    "\n",
    "# Impute missing FSA and neighbourhood name values in the validation set based on the modes\n",
    "# in the training set\n",
    "X_valid_imp_area = impute_missing_fsa_nn(X_valid_imp_area, mode_fsa, mode_nn, fsa_nn_group_data_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age group: modal imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert all columns to strings to avoid error with data type\n",
    "X_train_new = X_train_imp_area.applymap(str) # Convert blanks to 'nan'\n",
    "X_valid_new = X_valid_imp_area.applymap(str) \n",
    "\n",
    "# Imputer with most_frequent (modal) strategy (note this removes the original indices of the entries)\n",
    "imputer = SimpleImputer(missing_values='nan', strategy='most_frequent', verbose=1)\n",
    "X_train_imp = pd.DataFrame(imputer.fit_transform(X_train_new)) \n",
    "X_valid_imp = pd.DataFrame(imputer.transform(X_valid_new))\n",
    "\n",
    "# Rename columns\n",
    "X_train_imp.columns = X_train_imp_area.columns\n",
    "X_valid_imp.columns = X_valid_imp_area.columns\n",
    "\n",
    "# Put back the original indices\n",
    "X_train_imp.index = X_train_imp_area.index\n",
    "X_valid_imp.index = X_valid_imp_area.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsa_nn_group_data_sorted.to_excel(\"fsa_nn_group_data_sorted.xlsx\")\n",
    "# X_train_imp_area.to_excel(\"X_train_imp_area.xlsx\")\n",
    "# X_valid_imp_area.to_excel(\"X_valid_imp_area.xlsx\")\n",
    "# X_train_imp.to_excel(\"X_train_imp.xlsx\")\n",
    "# X_valid_imp.to_excel(\"X_valid_imp.xlsx\")\n",
    "# X_train_new.to_excel(\"X_train_new.xlsx\")\n",
    "# X_valid_new.to_excel(\"X_valid_new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values_table(X_train_imp)\n",
    "missing_values_table(X_valid_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection and Engineering\n",
    "\n",
    "Feature engineering will involve encoding the features:\n",
    "\n",
    "-- Outbreak Associated (2 values): one-hot encoding with binary values, equivalent to label encoding (0/1) to avoid the dummy variable trap\n",
    "\n",
    "-- Age Group (9 values): label encoding\n",
    "\n",
    "-- Neighbourhood Name and FSA: one-hot encoding with 'Other' column for rare cases\n",
    "\n",
    "-- Source of Infection (8 values): one-hot encoding\n",
    "\n",
    "-- Classification (2 values): one-hot encoding with binary values, equivalent to label encoding (0/1)\n",
    "\n",
    "-- Episode Date and Reported Date: encode month, date, day of the week as cyclical features (other possibilities: time since interesting event? holiday? season?)\n",
    "\n",
    "-- Client Gender (5 values): one-hot encoding\n",
    "\n",
    "-- Currently Hospitalized, Currently in ICU, Currently Intubated, Ever Hospitalized, Ever in ICU, Ever Intubated (2 values): one-hot encoding with binary values, equivalent to label encoding (0/1)\n",
    "\n",
    "CONSIDER SCALING AT THE END (FOR THE AGE GROUP)\n",
    "\n",
    "Feature selection will involve removing unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outbreak Associated, Age Group, Currently Hospitalized, Currently in ICU, Currently Intubated, \n",
    "# Ever Hospitalized, Ever in ICU, Ever Intubated: label encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Specify columns for label encoding\n",
    "lbl_enc_col = ['Outbreak Associated', 'Age Group', 'Classification', 'Currently Hospitalized', 'Currently in ICU', \\\n",
    "               'Currently Intubated', 'Ever Hospitalized', 'Ever in ICU', 'Ever Intubated']\n",
    "\n",
    "# Define the label encoder\n",
    "lbl_enc = LabelEncoder()\n",
    "\n",
    "# Make a copy\n",
    "X_train_lbl = X_train_imp.copy()\n",
    "X_valid_lbl = X_valid_imp.copy()\n",
    "\n",
    "# Loop through all of the specified columns\n",
    "for col in lbl_enc_col:\n",
    "    X_train_lbl[col] = lbl_enc.fit_transform(X_train_lbl[col])\n",
    "    X_valid_lbl[col] = lbl_enc.transform(X_valid_lbl[col])\n",
    "    \n",
    "# Put back indices\n",
    "X_train_lbl.index = X_train_imp.index\n",
    "X_valid_lbl.index = X_valid_imp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neighbourhood Name and FSA: one-hot encoding with 'Other' column for rare cases\n",
    "# # Based on Maya Gilad's tutorial here: https://medium.com/gett-engineering/handling-rare-categorical-values-in-pandas-d1e3f17475f0\n",
    "\n",
    "# # Value counts (number of cases associated with each FSA)\n",
    "# X_train_oh['FSA'].value_counts()\n",
    "# # Statistics\n",
    "# X_train_oh['FSA'].value_counts().describe()\n",
    "# # Normalized value counts\n",
    "# X_train_oh['FSA'].value_counts(normalize=True)\n",
    "# # Normalized value counts: statistics\n",
    "# X_train_oh['FSA'].value_counts(normalize=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Value counts (number of cases associated with each Neighbourhood Name)\n",
    "# X_train_oh['Neighbourhood Name'].value_counts()\n",
    "# # Statistics\n",
    "# X_train_oh['Neighbourhood Name'].value_counts().describe()\n",
    "# # Normalized value counts\n",
    "# X_train_oh['Neighbourhood Name'].value_counts(normalize=True)\n",
    "# # Normalized value counts: statistics\n",
    "# X_train_oh['Neighbourhood Name'].value_counts(normalize=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the threshold for specifying rare cases and the normalized value counts (in descending order)\n",
    "# Input: col_name: column name\n",
    "# Note that this is done on the training set only (X_train_oh)\n",
    "def get_threshold_and_norm_vc_desc(col_name):    \n",
    "    # Normalized value counts\n",
    "    norm_vc = X_train_oh[col_name].value_counts(normalize=True, ascending=True)\n",
    "\n",
    "    # Inferred threshold\n",
    "    # Having 10% of outcomes in the 'Other' category seems to be a good starting point, so I will use this\n",
    "    # to set the threshold\n",
    "    threshold = norm_vc[(norm_vc.cumsum() > 0.1).idxmax()]\n",
    "\n",
    "    # Normalized value counts (in descending order)\n",
    "    norm_vc_desc = X_train_oh[col_name].value_counts(normalize=True)\n",
    "    \n",
    "    return threshold, norm_vc_desc\n",
    "\n",
    "# Function to replace the column with rare cases marked as 'Other'\n",
    "# Input: data: either the training set (X_train_oh) or the validation set (X_valid_oh)\n",
    "#        thresh_fsa, thresh_nn: threshold for marking rare case (from the training set)\n",
    "#        norm_vc_desc_fsa, norm_vc_desc_nn: normalized value counts in descending order (from the training set)\n",
    "# Output: data with modified column\n",
    "def replace_rare_cases_in_col(data, thresh_fsa, thresh_nn, norm_vc_desc_fsa, norm_vc_desc_nn):\n",
    "    # FSA\n",
    "    # Map the column to its normalized value count\n",
    "    map_to_norm_vc_desc_fsa = data['FSA'].map(norm_vc_desc_fsa)\n",
    "    # Replace the column entry with 'Other' when the normalized value count is less than the threshold\n",
    "    data['FSA'] = data['FSA'].mask(map_to_norm_vc_desc_fsa < thresh_fsa, 'Other')\n",
    "\n",
    "    # Neighbourhood Name\n",
    "    # Map the column to its normalized value count\n",
    "    map_to_norm_vc_desc_nn = data['Neighbourhood Name'].map(norm_vc_desc_nn)\n",
    "    # Replace the column entry with 'Other' when the normalized value count is less than the threshold\n",
    "    data['Neighbourhood Name'] = data['Neighbourhood Name'].mask(map_to_norm_vc_desc_nn < thresh_nn, 'Other')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neighbourhood Name and FSA: one-hot encoding with 'Other' column for rare cases\n",
    "# # Based on Maya Gilad's tutorial here: https://medium.com/gett-engineering/handling-rare-categorical-values-in-pandas-d1e3f17475f0\n",
    "\n",
    "# FSA and neighbourhood name are treated separately\n",
    "\n",
    "# Make copies of the dataframes\n",
    "X_train_oh = X_train_lbl.copy()\n",
    "X_valid_oh = X_valid_lbl.copy()\n",
    "\n",
    "# Get thresholds and normalized value counts from the training set\n",
    "thresh_fsa, norm_vc_desc_fsa = get_threshold_and_norm_vc_desc('FSA')\n",
    "thresh_nn, norm_vc_desc_nn = get_threshold_and_norm_vc_desc('Neighbourhood Name')\n",
    "\n",
    "# Get training set and validation set with rare column entries listed as 'Other'\n",
    "X_train_oh = replace_rare_cases_in_col(X_train_oh, thresh_fsa, thresh_nn, norm_vc_desc_fsa, norm_vc_desc_nn)\n",
    "X_valid_oh = replace_rare_cases_in_col(X_valid_oh, thresh_fsa, thresh_nn, norm_vc_desc_fsa, norm_vc_desc_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine rare cases of the FSA and neighbourhood name\n",
    "# # A patient's location is defined by a combination of FSA and neighbourhood name.\n",
    "# # FSA and neighbourhood name are treated as a set, i.e. the number of occurrences of a particular combination of\n",
    "# # FSA and neighbourhood name are found. \n",
    "# # Otherwise, if the FSA and NN are treated independently, you may have cases where one of the FSA and NN are \n",
    "# # labelled 'Other', but not both.\n",
    "# # The threshold for determining 'rare' cases is such that at least 10% of cases are rare. \n",
    "\n",
    "# # Make a copy of the dataset\n",
    "# X_train_other = X_train_oh.copy()\n",
    "# X_valid_other = X_valid_oh.copy()\n",
    "\n",
    "# # For the training set: group data by FSA and neighbourhood and count number of occurrences\n",
    "# fsa_nn = X_train_other.groupby(['Neighbourhood Name', 'FSA'])['Outbreak Associated'].agg([len]).reset_index()\n",
    "# # Sort from lowest to highest count\n",
    "# fsa_nn_sorted = fsa_nn.sort_values(by='len', ascending=True)\n",
    "# # Normalize the value counts (or length)\n",
    "# norm_vc = fsa_nn_sorted['len']/(np.sum(fsa_nn_sorted['len']))\n",
    "# ## Map the normalized value counts to the sorted dataframe\n",
    "# #fsa_nn_map = fsa_nn_sorted['FSA'].map(norm_vc)\n",
    "# # Add a new column with normalized value counts\n",
    "# fsa_nn_sorted['norm_vc'] = norm_vc\n",
    "\n",
    "# # Inferred threshold\n",
    "# # Having 10% of outcomes in the 'Other' category seems to be a good starting point, so I will use this\n",
    "# # to set the threshold\n",
    "# thresh_fsa_nn = norm_vc.loc[(norm_vc.cumsum() > 0.1).idxmax()]\n",
    "\n",
    "# # Normalized value counts (in descending order)\n",
    "# fsa_nn_sorted_desc = fsa_nn_sorted.sort_values(by='norm_vc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to return dataframe with rare FSA and Neighbourhood Name cases as 'Other'\n",
    "# # Output: modified dataframe (either training set or validation set)\n",
    "# # Input: data: original dataframe (either training set or validation set)\n",
    "# #        fsa_nn_sorted_desc: dataframe with sorted FSA and NN value counts\n",
    "# #        thresh_fsa_nn: normalized-value-count threshold for 'Other' cases\n",
    "# def data_with_fsa_nn_other(data, fsa_nn_sorted_desc, thresh_fsa_nn):\n",
    "#     # Merge the training data with the normalized value counts\n",
    "#     norm_vc_merge = data[['Neighbourhood Name', 'FSA']].merge(fsa_nn_sorted_desc, how='left', indicator=True)['norm_vc']\n",
    "#     # Make the indices the same (norm_vc is ordered by index)\n",
    "#     norm_vc_merge.index = data.index\n",
    "#     # Add the norm_vc column to the training data\n",
    "#     data['norm_vc'] = norm_vc_merge\n",
    "#     # Replace the Neighbourhood Name and FSA with 'Other' when the normalized value count is less than the threshold\n",
    "#     data[['Neighbourhood Name', 'FSA']] = data[['Neighbourhood Name', 'FSA']].mask(data['norm_vc'] < thresh_fsa_nn, 'Other')\n",
    "#     # Remove norm_vc column\n",
    "#     data = data.drop(['norm_vc'], axis=1)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataframes with rare FSA and Neighbourhood Name cases as 'Other'\n",
    "# # Use normalized value counts and threshold for the FSA and NN from the training set and apply to the validation set\n",
    "# X_train_other = data_with_fsa_nn_other(X_train_other, fsa_nn_sorted_desc, thresh_fsa_nn)\n",
    "# X_valid_other = data_with_fsa_nn_other(X_valid_other, fsa_nn_sorted_desc, thresh_fsa_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbourhood Name, FSA, Source of Infection, Client Gender: one-hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Specify columns for one-hot encoding\n",
    "oh_col = ['Neighbourhood Name', 'FSA', 'Source of Infection', 'Client Gender']\n",
    "\n",
    "# One-hot encoder:\n",
    "# Return an array instead of a sparse matrix\n",
    "# Drop last value to avoid the dummy variable trap and also because the unknown columns aren't very helpful\n",
    "oh_enc = OneHotEncoder(handle_unknown='error', sparse=False, drop=[['Unknown/Missing'], ['UNKNOWN']])\n",
    "X_train_oh_col = pd.DataFrame(oh_enc.fit_transform(X_train_oh[oh_col]))\n",
    "X_valid_oh_col = pd.DataFrame(oh_enc.transform(X_valid_oh[oh_col]))\n",
    "\n",
    "# Put back the indices\n",
    "X_train_oh_col.index = X_train_lbl.index\n",
    "X_valid_oh_col.index = X_valid_lbl.index\n",
    "\n",
    "# # Make copies of the dataframes\n",
    "# X_train_oh = X_train_lbl.copy()\n",
    "# X_valid_oh = X_valid_lbl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename one-hot encoded columns, dropping last value\n",
    "# Output: return dataframe with renamed columns\n",
    "# Input: X_train_oh_col or X_valid_oh_col (one-hot encoded columns);\n",
    "#        oh_enc (one-hot encoder)\n",
    "def rename_col_oh(X_oh_col, oh_enc):\n",
    "    X_oh_col.columns = np.concatenate((oh_enc.categories_[0][:-1], oh_enc.categories_[1][:-1]))\n",
    "    return X_oh_col\n",
    "\n",
    "# Function to put the one-hot encoded columns back into the dataframe\n",
    "# Keep the original order for now\n",
    "# Output: return ordered dataframe\n",
    "# Input: X_train_oh or X_valid_oh (dataframe to modify); X_train_oh_col or X_valid_oh_col (one-hot encoded columns);\n",
    "#        oh_enc (one-hot encoder)\n",
    "def order_col_oh(X_oh, X_oh_col, oh_enc):\n",
    "    X_oh = pd.concat([X_oh.loc[:,'Outbreak Associated':'FSA'], \\\n",
    "    X_oh_col.loc[:,oh_enc.categories_[0][0]:oh_enc.categories_[0][-2]], # Source of Infection\n",
    "    X_oh.loc[:,'Classification':'Reported Date'], \\\n",
    "    X_oh_col.loc[:,oh_enc.categories_[1][0]:oh_enc.categories_[1][-2]], # Client Gender\n",
    "    X_oh.loc[:,'Currently Hospitalized':'Ever Intubated']], axis=1, sort=False)\n",
    "    return X_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename one-hot encoded columns, dropping last value\n",
    "X_train_oh_col = rename_col_oh(X_train_oh_col, oh_enc)\n",
    "X_valid_oh_col = rename_col_oh(X_valid_oh_col, oh_enc)\n",
    "\n",
    "# Put the one-hot encoded columns back into the dataframe\n",
    "X_train_oh = order_col_oh(X_train_oh, X_train_oh_col, oh_enc)\n",
    "X_valid_oh = order_col_oh(X_valid_oh, X_valid_oh_col, oh_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for categorical variables\n",
    "covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the patient ID, the columns are all categorical variables. I will deal with encoding for categorical variables in the modelling section."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
