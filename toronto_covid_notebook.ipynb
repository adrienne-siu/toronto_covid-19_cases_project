{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto COVID-19 Cases Project\n",
    "\n",
    "Author: Adrienne Siu\n",
    "Date: August 2020\n",
    "\n",
    "# 1. Problem Definition\n",
    "\n",
    "In this project, I will use machine learning to:\n",
    "(1) Predict the outcomes of cases of COVID-19 in Toronto\n",
    "(2) Find the variables that correlate most with the outcome\n",
    "\n",
    "The dataset is available on Toronto Open Data and the version from July 29, 2020 was used: https://open.toronto.ca/dataset/covid-19-cases-in-toronto/\n",
    "\n",
    "This dataset has been saved as 'COVID19 cases.csv'.\n",
    "\n",
    "The three possible outcomes are: fatal, resolved (not fatal), and active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "%qtconsole\n",
    "import pdb\n",
    "#import contextlib\n",
    "#with contextlib.redirect_stdout(None):\n",
    "#import pixiedust\n",
    "\n",
    "#%%pixie_debugger\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, I will explore the data, including correlations between variables and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of dataset file (.csv)\n",
    "covid_file_path = 'COVID19 cases.csv'\n",
    "\n",
    "# Read the file\n",
    "covid_data = pd.read_csv(covid_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain filtered entries in dataset\n",
    "# Input: column_name is a string for the name of the column, e.g. 'Outbreak', 'Age Group'\n",
    "#        filter_column is a string for the column to be filtered\n",
    "#        filter_entry is a string for the entry to be filtered\n",
    "# Ex. To filter only the fatal outcomes, filter_column = 'Outcome' and filter_entry = 'FATAL'\n",
    "# Returns entries and value counts for the specific column after filtering\n",
    "def get_filtered_entries_and_value_counts_from_column(column_name, filter_column, filter_entry):\n",
    "    # Get the count of each unique entry (ordered by name)\n",
    "    entry_vc = covid_data[covid_data[filter_column]==filter_entry][column_name].value_counts() #.sort_index()\n",
    "    # Get the unique entries and put them in a list\n",
    "    entry = entry_vc.index.tolist()    \n",
    "    return entry, entry_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a bar chart of a column variable with filters applied\n",
    "# Stack by outcome: active, fatal, resolved\n",
    "# Input: column_name is a string for the name of the column, e.g. 'Outbreak', 'Age Group'\n",
    "#        filter_column is a string for the column to be filtered\n",
    "#        filter_entry_list is a list of strings for entries to be filtered\n",
    "def create_filtered_bar_chart(column_name, filter_column, filter_entry_list, fig_size):\n",
    "    # Initialize lists of entries and value counts after filtering\n",
    "    entry_list = []\n",
    "    entry_vc_list = []\n",
    "    \n",
    "    # Figure size\n",
    "    if fig_size == 'large':\n",
    "        fig = plt.figure(figsize=(20,30))\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "\n",
    "    # Loop through entries to be filtered\n",
    "    for i in range(len(filter_entry_list)):\n",
    "        # Entries and value counts for each filter\n",
    "        entry, entry_vc = get_filtered_entries_and_value_counts_from_column(column_name, filter_column, filter_entry_list[i])\n",
    "        # Append to list\n",
    "        entry_list.append(entry)\n",
    "        entry_vc_list.append(entry_vc)\n",
    "        # Create a bar stacked for each filtered entry, e.g. 'Fatal', 'Active', 'Resolved'\n",
    "        plt.barh(entry_list[i], entry_vc_list[i])\n",
    "\n",
    "    # Invert the y-axis so that the order of the entries is from top to bottom\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title('Number of COVID-19 Cases By ' + column_name)\n",
    "    plt.legend(filter_entry_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the column names of the dataset into a list\n",
    "columns = covid_data.columns\n",
    "\n",
    "# Remove the first two column names (the IDs) because I don't need to graph them\n",
    "columns_no_IDs = columns[2:]\n",
    "\n",
    "# Column to be filtered\n",
    "filter_column = 'Outcome'\n",
    "# Entries to be filtered:\n",
    "# First make stacked bar charts of all three outcomes, then bar charts of only fatal outcomes\n",
    "filter_entry_list = [['FATAL', 'RESOLVED', 'ACTIVE'], ['FATAL']]\n",
    "\n",
    "for j in range(len(filter_entry_list)):\n",
    "    # Create bar charts for relevant columns\n",
    "    for column in columns_no_IDs:\n",
    "        #create_bar_chart(column)\n",
    "        # Figure size\n",
    "        if column == 'Neighbourhood Name' or column == 'FSA' or column == 'Episode Date' or column == 'Reported Date':\n",
    "            fig_size = 'large'\n",
    "        else:\n",
    "            fig_size = 'normal'\n",
    "\n",
    "        create_filtered_bar_chart(column, filter_column, filter_entry_list[j], fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fatal cases make up 7.5% of total cases. As the number of resolved cases is an order of a magnitude higher, it is difficult to see the fatal cases in the bar charts. Hence, the fatal cases are graphed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "In this section, I will clean the data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target: outcome\n",
    "y = covid_data.Outcome\n",
    "\n",
    "# Predictor: drop _id and Assigned_ID columns\n",
    "X = covid_data.drop(['_id', 'Assigned_ID'], axis=1)\n",
    "\n",
    "# Split into training and validation sets (80/20 split)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a table of missing values in columns\n",
    "# Function written by Nikos Tavoularis on Stack Overflow: https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe/39734251#39734251\n",
    "# Function comments by AS\n",
    "def missing_values_table(df):\n",
    "        # Count the number of missing values in each column\n",
    "        mis_val = df.isnull().sum()\n",
    "        # Calculate the percentage of missing values in each column\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        # Put the number of missing values and % missing values in a table\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        # Rename the columns of the table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        # Take out columns with no missing values\n",
    "        # Reorder columns with missing values in descending order\n",
    "        # Round % missing values to 1 decimal place\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        # Print summary\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        # Return table of missing values\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create table of missing values\n",
    "missing_values_table(covid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, there are only 3/18 columns with missing values. For each column, the % missing values is 4% or less. Hence, the dataset is fairly complete.\n",
    "\n",
    "I will fill some of the missing values:\n",
    "\n",
    "The age group will be filled with modal imputation, i.e. the modal age group.\n",
    "\n",
    "If neither of the FSA or the neighbourhood name are known, these will remain unknown.\n",
    "\n",
    "If the neighbourhood name is known but not the FSA, the FSA will be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new copies of datasets\n",
    "# y doesn't have any missing values, so imputation is not needed\n",
    "X_train_imp_area = X_train\n",
    "X_valid_imp_area = X_valid\n",
    "\n",
    "data = X_train_imp_area\n",
    "\n",
    "# If both the neighbourhood and FSA are blank, find the modal neighbourhood and FSA (as a pair)\n",
    "# Group data by FSA and neighbourhood and count number of occurrences\n",
    "fsa_nn_group_data = data.groupby(['FSA', 'Neighbourhood Name'])['Outbreak Associated'].agg([len]).reset_index()\n",
    "# # Sort from highest to lowest count (i.e. the mode is the first value)\n",
    "fsa_nn_group_data_sorted = fsa_nn_group_data.sort_values(by='len', ascending=False)\n",
    "print(\"Done\")\n",
    "# Check for missing values for the neighbourhood name and FSA\n",
    "# Note there are no cases when the neighbourhood name is known but the FSA is blank\n",
    "# If an entry is blank, it is a 'NaN' and hence a float\n",
    "# If an entry is not blank, it is a string\n",
    "#for i in data.index:\n",
    "     # If both the neighbourhood name and FSA are blank\n",
    "#     if type(data['Neighbourhood Name'][i]) == float and type(data['FSA'][i]) == float:\n",
    "         # Use the modal FSA and neighbourhood name\n",
    "#         data['FSA'][i] = fsa_nn_group_data_sorted.iloc[0]['FSA']\n",
    "#         data['Neighbourhood Name'][i] = fsa_nn_group_data_sorted.iloc[0]['Neighbourhood Name']\n",
    "#         print(\"Instance 1\", i, data['FSA'][i], data['Neighbourhood Name'][i])\n",
    "    \n",
    "     # If the neighbourhood name is blank but the FSA is known\n",
    "#     elif type(data['Neighbourhood Name'][i]) == float and type(data['FSA'][i]) != float:\n",
    "#         # Find the modal neighbourhood name for the given FSA\n",
    "#         # i.e. find the first instance of FSA in fsa_nn_group_data_sorted and its associated neighbourhood name\n",
    "#         mode_nn = fsa_nn_group_data_sorted.loc[fsa_nn_group_data_sorted['FSA'] == data['FSA'][i]].iloc[0]\n",
    "#         data['Neighbourhood Name'][i] = mode_nn\n",
    "#         print(\"Instance 2\", i, data['FSA'][i], data['Neighbourhood Name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age group: modal imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer with most_frequent (modal) strategy\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train_imp_age = pd.DataFrame(imputer.fit_transform(X_train_imp_area))\n",
    "X_valid_imp_age = pd.DataFrame(imputer.transform(X_valid_imp_area))\n",
    "\n",
    "# Rename columns\n",
    "X_train_imp_age.columns = X_train_imp_area.columns\n",
    "X_valid_imp_age.columns = X_valid_imp_area.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection and Engineering\n",
    "\n",
    "Feature selection will involve:\n",
    "-Removing the _id and Assigned_ID columns\n",
    "\n",
    "Feature engineering will involve:\n",
    "-Label encoding the Age Group, Episode Date, and Reported Date columns\n",
    "-One-hot encoding the other columns\n",
    "\n",
    "Note that one-hot encoding the Neighbourhood Name and FSA columns will yield sparse data. We will see how the models fare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for categorical variables\n",
    "covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the patient ID, the columns are all categorical variables. I will deal with encoding for categorical variables in the modelling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
